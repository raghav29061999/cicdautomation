import json
import requests
from bs4 import BeautifulSoup, Tag

def scrape_dataiku_doc(url: str, outfile: str = None, timeout: int = 20) -> dict:
    """
    Scrape Dataiku documentation page content into nested JSON structure.
    Ignores left nav panel, extracts only main content (headings + body text).
    """
    resp = requests.get(url, timeout=timeout, headers={
        "User-Agent": "Mozilla/5.0 (compatible; DataikuScraper/1.0)"
    })
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    # ---- Focus only on main content container ----
    # On Dataiku docs, it's usually <div role="main"> or class="wy-nav-content"
    main = soup.find("div", {"role": "main"}) or soup.find("div", class_="wy-nav-content")
    if not main:
        main = soup.body  # fallback

    # Collect all headings and paragraphs under main
    results = {}
    current_h1, current_h2, current_h3 = None, None, None

    for tag in main.find_all(["h1", "h2", "h3", "h4", "h5", "h6", "p", "ul", "ol"], recursive=True):
        if isinstance(tag, Tag):
            text = tag.get_text(" ", strip=True)

            # Handle headings
            if tag.name == "h1":
                current_h1 = text
                results[current_h1] = {}
                current_h2 = current_h3 = None
            elif tag.name == "h2" and current_h1:
                current_h2 = text
                results[current_h1][current_h2] = {}
                current_h3 = None
            elif tag.name == "h3" and current_h1 and current_h2:
                current_h3 = text
                results[current_h1][current_h2][current_h3] = ""
            # Handle paragraph/list text
            elif tag.name in {"p", "ul", "ol"}:
                clean_txt = tag.get_text("\n", strip=True)
                if current_h3:
                    # lowest level -> append text
                    results[current_h1][current_h2][current_h3] += "\n" + clean_txt
                elif current_h2:
                    results[current_h1][current_h2][text] = clean_txt
                elif current_h1:
                    results[current_h1][text] = clean_txt

    # Save to file if requested
    if outfile:
        with open(outfile, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

    return results


if __name__ == "__main__":
    url = "https://doc.dataiku.com/dss/latest/explore/"  # example
    data = scrape_dataiku_doc(url, "dataiku_scraped.json")
    print(json.dumps(data, indent=2, ensure_ascii=False))
