import json
import requests
from bs4 import BeautifulSoup, Tag

def scrape_dataiku_doc(url: str, outfile: str = None, timeout: int = 20) -> dict:
    """
    Scrape Dataiku documentation page content into nested JSON structure.
    Ignores left nav panel, extracts only main content (headings + body text).
    """
    resp = requests.get(url, timeout=timeout, headers={
        "User-Agent": "Mozilla/5.0 (compatible; DataikuScraper/1.0)"
    })
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    # ---- Focus only on main content container ----
    # On Dataiku docs, it's usually <div role="main"> or class="wy-nav-content"
    main = soup.find("div", {"role": "main"}) or soup.find("div", class_="wy-nav-content")
    if not main:
        main = soup.body  # fallback

    # Collect all headings and paragraphs under main
    results = {}
    current_h1, current_h2, current_h3 = None, None, None

    for tag in main.find_all(["h1", "h2", "h3", "h4", "h5", "h6", "p", "ul", "ol"], recursive=True):
        if isinstance(tag, Tag):
            text = tag.get_text(" ", strip=True)

            # Handle headings
            if tag.name == "h1":
                current_h1 = text
                results[current_h1] = {}
                current_h2 = current_h3 = None
            elif tag.name == "h2" and current_h1:
                current_h2 = text
                results[current_h1][current_h2] = {}
                current_h3 = None
            elif tag.name == "h3" and current_h1 and current_h2:
                current_h3 = text
                results[current_h1][current_h2][current_h3] = ""
            # Handle paragraph/list text
            elif tag.name in {"p", "ul", "ol"}:
                clean_txt = tag.get_text("\n", strip=True)
                if current_h3:
                    # lowest level -> append text
                    results[current_h1][current_h2][current_h3] += "\n" + clean_txt
                elif current_h2:
                    results[current_h1][current_h2][text] = clean_txt
                elif current_h1:
                    results[current_h1][text] = clean_txt

    # Save to file if requested
    if outfile:
        with open(outfile, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

    return results


if __name__ == "__main__":
    url = "https://doc.dataiku.com/dss/latest/explore/"  # example
    data = scrape_dataiku_doc(url, "dataiku_scraped.json")
    print(json.dumps(data, indent=2, ensure_ascii=False))





--


failing due to a NumberFormatException, which occurs because a field expected to be a standard long integer is receiving a number with 20 digits, exceeding its maximum capacity.

This is likely caused by an unexpected, large value in the source data at line 113311, column 41.

Here are two possible ways to resolve this:

Immediate Resolution (Data Fix): We can manually identify and correct the specific invalid data entry (leaf_id at the specified location) to unblock the current job run.

Long-Term Resolution (Code Change): We can update the schema or code to use a data type that can handle a larger range of numbers, such as a BigInteger or equivalent. This will prevent similar errors from occurring in the future with new data.
